{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./parkinsons_data-og.csv')\n",
    "\n",
    "# Display first few rows and data types\n",
    "data = data.drop(columns=['name'])\n",
    "print(data.head())\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Example strategy to fill missing values (if any)\n",
    "data.fillna(data.median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize continuous columns (example using 4 bins for each feature)\n",
    "for col in data.columns:\n",
    "    if data[col].dtype != 'object':\n",
    "        data[col] = pd.cut(data[col], bins=4, labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical columns (if any)\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch, BicScore\n",
    "\n",
    "# Using Hill Climb Search with BIC score\n",
    "hc = HillClimbSearch(data)\n",
    "model = hc.estimate(scoring_method=BicScore(data))\n",
    "print(model.edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "# Define the Bayesian Network model based on learned structure\n",
    "bayesian_model = BayesianNetwork(model.edges())\n",
    "\n",
    "# Learn CPTs\n",
    "bayesian_model.fit(data, estimator=MaximumLikelihoodEstimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Inference\n",
    "inference = VariableElimination(bayesian_model)\n",
    "query = inference.query(variables=['status'], evidence={'MDVP:Fo(Hz)': 1})  # Example query\n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score,\n",
    "    brier_score_loss\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "from pgmpy.estimators import HillClimbSearch, BDeuScore, MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./parkinsons_data-og.csv')\n",
    "\n",
    "data = data.drop(columns=['name'])\n",
    "\n",
    "# Preprocess data: Handle missing values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Discretize continuous variables (example with 4 bins for each feature)\n",
    "n_bins = 4\n",
    "for col in data.columns:\n",
    "    if data[col].dtype != 'object':\n",
    "        data[col] = pd.cut(data[col], bins=n_bins, labels=False)\n",
    "\n",
    "# Encode categorical columns if any (convert to integer categories)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# To store results for all metrics\n",
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "f1_scores = []\n",
    "roc_aucs = []\n",
    "brier_scores = []\n",
    "kl_divergences = []\n",
    "max_iter = 200000000000\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data = data.iloc[train_index]\n",
    "    test_data = data.iloc[test_index]\n",
    "\n",
    "    # Structure learning using Hill Climb and BDeu score\n",
    "    hc = HillClimbSearch(train_data)\n",
    "    model = hc.estimate(scoring_method=BDeuScore(train_data), max_iter=max_iter)  # Use BDeu score\n",
    "    \n",
    "    # Define Bayesian Network with learned structure and fit CPTs\n",
    "    bayesian_model = BayesianNetwork(model.edges())\n",
    "    bayesian_model.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "    \n",
    "    # Inference setup\n",
    "    inference = VariableElimination(bayesian_model)\n",
    "    \n",
    "    # Lists to store true and predicted values for metrics\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    # Testing the model\n",
    "    for _, row in test_data.iterrows():\n",
    "        # Example: Querying the 'status' variable\n",
    "        evidence = row.drop('status').to_dict()  # Modify target variable if needed\n",
    "        true_status = row['status']\n",
    "        \n",
    "        # Filter evidence to only include values within valid states for each variable\n",
    "        valid_evidence = {}\n",
    "        for var, value in evidence.items():\n",
    "            if value < n_bins:  # Check if within expected range\n",
    "                valid_evidence[var] = value\n",
    "        \n",
    "        # Perform inference to predict 'status'\n",
    "        try:\n",
    "            query_result = inference.map_query(variables=['status'], evidence=valid_evidence)\n",
    "            # Store the true and predicted status\n",
    "            y_true.append(true_status)\n",
    "            y_pred.append(query_result['status'])\n",
    "\n",
    "            try:\n",
    "                # Perform the probability query\n",
    "                prob_result = inference.query(variables=['status'], evidence=valid_evidence)\n",
    "                prob_demented = prob_result.values[1]  # Probability of class 1\n",
    "                y_prob.append(prob_demented)\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError for evidence: {valid_evidence}. Missing key: {e}\")\n",
    "                y_prob.append(0.0000001)  # Append a very small value to avoid NaN\n",
    "            \n",
    "        except IndexError:\n",
    "            # Skip this instance if evidence is out of range\n",
    "            continue\n",
    "\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # Check for NaN values and filter out any rows with NaNs\n",
    "    valid_indices = ~np.isnan(y_true) & ~np.isnan(y_prob)\n",
    "    y_true_clean = y_true[valid_indices]\n",
    "    y_pred_clean = y_pred[valid_indices]\n",
    "    y_prob_clean = y_prob[valid_indices]\n",
    "\n",
    "    # Check the unique values in y_true_clean and y_pred_clean to determine pos_label\n",
    "    unique_labels = set(y_true_clean) | set(y_pred_clean)\n",
    "    pos_label = 1 if 1 in unique_labels else min(unique_labels)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracies.append(accuracy_score(y_true_clean, y_pred_clean))\n",
    "    balanced_accuracies.append(balanced_accuracy_score(y_true_clean, y_pred_clean))\n",
    "    f1_scores.append(f1_score(y_true_clean, y_pred_clean, pos_label=pos_label))\n",
    "    \n",
    "    if len(y_prob_clean) > 0:  # Ensure there are valid probabilities\n",
    "        roc_aucs.append(roc_auc_score(y_true_clean, y_prob_clean))\n",
    "        brier_scores.append(brier_score_loss(y_true_clean, y_prob_clean))\n",
    "        \n",
    "        # Calculate KL Divergence for this fold\n",
    "        # Add a small constant to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        prob_true = np.array([p if y == pos_label else 1 - p for y, p in zip(y_true_clean, y_prob_clean)])\n",
    "        prob_true = np.clip(prob_true, epsilon, None)  # Ensure no zero probabilities\n",
    "        y_prob_clean = np.clip(y_prob_clean, epsilon, None)  # Ensure no zero probabilities\n",
    "\n",
    "        kl_div = entropy(prob_true, y_prob_clean, base=2)\n",
    "        kl_divergences.append(kl_div)\n",
    "\n",
    "# Average the results across folds\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Balanced Accuracy: {np.mean(balanced_accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"ROC AUC: {np.mean(roc_aucs):.4f}\")\n",
    "print(f\"Brier Score: {np.mean(brier_scores):.4f}\")\n",
    "print(f\"KL Divergence: {np.mean(kl_divergences):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Balanced Accuracy: {np.mean(balanced_accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"ROC AUC: {np.mean(roc_aucs):.4f}\")\n",
    "print(f\"Brier Score: {np.mean(brier_scores):.4f}\")\n",
    "print(f\"KL Divergence: {np.mean(kl_divergences):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score,\n",
    "    brier_score_loss\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "from pgmpy.estimators import HillClimbSearch, BDeuScore, MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./parkinsons_data-og.csv')\n",
    "\n",
    "data = data.drop(columns=['name'])\n",
    "\n",
    "# Preprocess data: Handle missing values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Discretize continuous variables (example with 4 bins for each feature)\n",
    "n_bins = 4\n",
    "for col in data.columns:\n",
    "    if data[col].dtype != 'object':\n",
    "        data[col] = pd.cut(data[col], bins=n_bins, labels=False)\n",
    "\n",
    "# Encode categorical columns if any (convert to integer categories)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# To store results for all metrics\n",
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "f1_scores = []\n",
    "roc_aucs = []\n",
    "brier_scores = []\n",
    "kl_divergences = []\n",
    "max_iter = 200000000000\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data = data.iloc[train_index]\n",
    "    test_data = data.iloc[test_index]\n",
    "\n",
    "    # Structure learning using Hill Climb and BDeu score\n",
    "    hc = HillClimbSearch(train_data)\n",
    "    model = hc.estimate(scoring_method=BDeuScore(train_data), max_iter=max_iter)  # Use BDeu score\n",
    "    \n",
    "    # Define Bayesian Network with learned structure and fit CPTs\n",
    "    bayesian_model = BayesianNetwork(model.edges())\n",
    "    bayesian_model.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "    \n",
    "    # Inference setup\n",
    "    inference = VariableElimination(bayesian_model)\n",
    "    \n",
    "    # Lists to store true and predicted values for metrics\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    # Testing the model\n",
    "    for _, row in test_data.iterrows():\n",
    "        # Example: Querying the 'status' variable\n",
    "        evidence = row.drop('status').to_dict()  # Modify target variable if needed\n",
    "        true_status = row['status']\n",
    "        \n",
    "        # Filter evidence to only include values within valid states for each variable\n",
    "        valid_evidence = {}\n",
    "        for var, value in evidence.items():\n",
    "            if value < n_bins:  # Check if within expected range\n",
    "                valid_evidence[var] = value\n",
    "        \n",
    "        # Perform inference to predict 'status'\n",
    "        try:\n",
    "            query_result = inference.map_query(variables=['status'], evidence=valid_evidence)\n",
    "            # Store the true and predicted status\n",
    "            y_true.append(true_status)\n",
    "            y_pred.append(query_result['status'])\n",
    "\n",
    "            try:\n",
    "                # Perform the probability query\n",
    "                prob_result = inference.query(variables=['status'], evidence=valid_evidence)\n",
    "                prob_demented = prob_result.values[1]  # Probability of class 1\n",
    "                y_prob.append(prob_demented)\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError for evidence: {valid_evidence}. Missing key: {e}\")\n",
    "                y_prob.append(0.0000001)  # Append a very small value to avoid NaN\n",
    "            \n",
    "        except IndexError:\n",
    "            # Skip this instance if evidence is out of range\n",
    "            continue\n",
    "\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # Check for NaN values and filter out any rows with NaNs\n",
    "    valid_indices = ~np.isnan(y_true) & ~np.isnan(y_prob)\n",
    "    y_true_clean = y_true[valid_indices]\n",
    "    y_pred_clean = y_pred[valid_indices]\n",
    "    y_prob_clean = y_prob[valid_indices]\n",
    "\n",
    "    # Check the unique values in y_true_clean and y_pred_clean to determine pos_label\n",
    "    unique_labels = set(y_true_clean) | set(y_pred_clean)\n",
    "    pos_label = 1 if 1 in unique_labels else min(unique_labels)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracies.append(accuracy_score(y_true_clean, y_pred_clean))\n",
    "    balanced_accuracies.append(balanced_accuracy_score(y_true_clean, y_pred_clean))\n",
    "    f1_scores.append(f1_score(y_true_clean, y_pred_clean, pos_label=pos_label))\n",
    "    \n",
    "    if len(y_prob_clean) > 0:  # Ensure there are valid probabilities\n",
    "        roc_aucs.append(roc_auc_score(y_true_clean, y_prob_clean))\n",
    "        brier_scores.append(brier_score_loss(y_true_clean, y_prob_clean))\n",
    "        \n",
    "        # Calculate KL Divergence for this fold\n",
    "        # Add a small constant to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        prob_true = np.array([p if y == pos_label else 1 - p for y, p in zip(y_true_clean, y_prob_clean)])\n",
    "        prob_true = np.clip(prob_true, epsilon, None)  # Ensure no zero probabilities\n",
    "        y_prob_clean = np.clip(y_prob_clean, epsilon, None)  # Ensure no zero probabilities\n",
    "\n",
    "        kl_div = entropy(prob_true, y_prob_clean, base=2)\n",
    "        kl_divergences.append(kl_div)\n",
    "\n",
    "# Average the results across folds\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Balanced Accuracy: {np.mean(balanced_accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"ROC AUC: {np.mean(roc_aucs):.4f}\")\n",
    "print(f\"Brier Score: {np.mean(brier_scores):.4f}\")\n",
    "print(f\"KL Divergence: {np.mean(kl_divergences):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeedc1fa81f14a2fa2e72ed229c0e101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\anaconda3\\envs\\AML\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:489: RuntimeWarning: invalid value encountered in divide\n",
      "  phi.values = phi.values / phi.values.sum()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2e58b5f8ba4f37ae7f243b5eefd6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c32c3323bcb442c9bcd9c65520b4811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\anaconda3\\envs\\AML\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:489: RuntimeWarning: invalid value encountered in divide\n",
      "  phi.values = phi.values / phi.values.sum()\n",
      "c:\\Users\\rohit\\anaconda3\\envs\\AML\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:489: RuntimeWarning: invalid value encountered in divide\n",
      "  phi.values = phi.values / phi.values.sum()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ded9f058f9d4aabb26bc23ec57808e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fc7bf1d35d452f92248d7e0095805c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\anaconda3\\envs\\AML\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:489: RuntimeWarning: invalid value encountered in divide\n",
      "  phi.values = phi.values / phi.values.sum()\n",
      "WARNING:pgmpy:Found unknown state name. Trying to switch to using all state names as state numbers\n",
      "WARNING:pgmpy:Found unknown state name. Trying to switch to using all state names as state numbers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33be175a3ed4650aaab5090d34b4bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc4f7b95c1745d8a4c4a33c990a6943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafc586498564a25b330e9ba81e7085e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8474\n",
      "Balanced Accuracy: 0.7551\n",
      "F1 Score: 0.6412\n",
      "ROC AUC: 0.8427\n",
      "Brier Score: 0.1218\n",
      "KL Divergence: 3.6904\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score,\n",
    "    brier_score_loss\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "from pgmpy.estimators import HillClimbSearch, BDeuScore, MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./parkinsons_data-og.csv')\n",
    "\n",
    "data = data.drop(columns=['name'])\n",
    "\n",
    "# Preprocess data: Handle missing values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Discretize continuous variables (example with 4 bins for each feature)\n",
    "n_bins = 4\n",
    "for col in data.columns:\n",
    "    if data[col].dtype != 'object':\n",
    "        data[col] = pd.cut(data[col], bins=n_bins, labels=False)\n",
    "\n",
    "# Encode categorical columns if any (convert to integer categories)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=8, shuffle=True, random_state=51)\n",
    "\n",
    "# To store results for all metrics\n",
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "f1_scores = []\n",
    "roc_aucs = []\n",
    "brier_scores = []\n",
    "kl_divergences = []\n",
    "max_iter = 200000000000\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data = data.iloc[train_index]\n",
    "    test_data = data.iloc[test_index]\n",
    "\n",
    "    # Structure learning using Hill Climb and BDeu score\n",
    "    hc = HillClimbSearch(train_data)\n",
    "    model = hc.estimate(scoring_method=BDeuScore(train_data), max_iter=max_iter)  # Use BDeu score\n",
    "    \n",
    "    # Define Bayesian Network with learned structure and fit CPTs\n",
    "    bayesian_model = BayesianNetwork(model.edges())\n",
    "    bayesian_model.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "    \n",
    "    # Inference setup\n",
    "    inference = VariableElimination(bayesian_model)\n",
    "    with open('./bayesian_model.pkl', 'wb') as f:\n",
    "        pickle.dump(bayesian_model, f)\n",
    "    # Lists to store true and predicted values for metrics\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    # Testing the model\n",
    "    for _, row in test_data.iterrows():\n",
    "        # Example: Querying the 'status' variable\n",
    "        evidence = row.drop('status').to_dict()  # Modify target variable if needed\n",
    "        true_status = row['status']\n",
    "        \n",
    "        # Filter evidence to only include values within valid states for each variable\n",
    "        valid_evidence = {}\n",
    "        for var, value in evidence.items():\n",
    "            if value < n_bins:  # Check if within expected range\n",
    "                valid_evidence[var] = value\n",
    "        \n",
    "        # Perform inference to predict 'status'\n",
    "        try:\n",
    "            query_result = inference.map_query(variables=['status'], evidence=valid_evidence,show_progress=False)\n",
    "            # Store the true and predicted status\n",
    "            y_true.append(true_status)\n",
    "            y_pred.append(query_result['status'])\n",
    "\n",
    "            # try:\n",
    "                # Perform the probability query\n",
    "            # print(\"true_status\",true_status)\n",
    "            # print(\"valid_evidence\",valid_evidence)\n",
    "            prob_result = inference.query(variables=['status'], evidence=valid_evidence, show_progress=False)\n",
    "            prob_demented = prob_result.values[1]  # Probability of class 1\n",
    "            y_prob.append(prob_demented)\n",
    "            # except KeyError as e:\n",
    "            #     print(f\"KeyError for evidence: {valid_evidence}. Missing key: {e}\")\n",
    "            #     y_prob.append(0.0000001)  # Append a very small value to avoid NaN\n",
    "            \n",
    "        except IndexError:\n",
    "            # Skip this instance if evidence is out of range\n",
    "            continue\n",
    "\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "    \n",
    "\n",
    "    # Check for NaN values and filter out any rows with NaNs\n",
    "    valid_indices = ~np.isnan(y_true) & ~np.isnan(y_prob)\n",
    "    y_true_clean = y_true[valid_indices]\n",
    "    y_pred_clean = y_pred[valid_indices]\n",
    "    y_prob_clean = y_prob[valid_indices]\n",
    "\n",
    "    # Check the unique values in y_true_clean and y_pred_clean to determine pos_label\n",
    "    unique_labels = set(y_true_clean) | set(y_pred_clean)\n",
    "    pos_label = 1 if 1 in unique_labels else min(unique_labels)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracies.append(accuracy_score(y_true_clean, y_pred_clean))\n",
    "    balanced_accuracies.append(balanced_accuracy_score(y_true_clean, y_pred_clean))\n",
    "    f1_scores.append(f1_score(y_true_clean, y_pred_clean, pos_label=pos_label))\n",
    "    \n",
    "    if len(y_prob_clean) > 0:  # Ensure there are valid probabilities\n",
    "        roc_aucs.append(roc_auc_score(y_true_clean, y_prob_clean))\n",
    "        brier_scores.append(brier_score_loss(y_true_clean, y_prob_clean))\n",
    "        \n",
    "        # Calculate KL Divergence for this fold\n",
    "        # Add a small constant to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        prob_true = np.array([p if y == pos_label else 1 - p for y, p in zip(y_true_clean, y_prob_clean)])\n",
    "        prob_true = np.clip(prob_true, epsilon, None)  # Ensure no zero probabilities\n",
    "        y_prob_clean = np.clip(y_prob_clean, epsilon, None)  # Ensure no zero probabilities\n",
    "\n",
    "        kl_div = entropy(prob_true, y_prob_clean, base=2)\n",
    "        kl_divergences.append(kl_div)\n",
    "\n",
    "# Average the results across folds\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Balanced Accuracy: {np.mean(balanced_accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"ROC AUC: {np.mean(roc_aucs):.4f}\")\n",
    "print(f\"Brier Score: {np.mean(brier_scores):.4f}\")\n",
    "print(f\"KL Divergence: {np.mean(kl_divergences):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     prob_result \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mquery(variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m], evidence\u001b[38;5;241m=\u001b[39mevidence)\n\u001b[1;32m---> 76\u001b[0m     prob_status_1 \u001b[38;5;241m=\u001b[39m \u001b[43mprob_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Probability of status=1\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP(status=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | evidence) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob_status_1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 2"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Load the trained Bayesian model\n",
    "with open('bayesian_model.pkl', 'rb') as f:\n",
    "    bayesian_model = pickle.load(f)\n",
    "\n",
    "# Set up inference\n",
    "inference = VariableElimination(bayesian_model)\n",
    "\n",
    "# Define your evidence with continuous values\n",
    "# for status =1 \n",
    "status = 1\n",
    "evidence = {\n",
    "    'MDVP:Fo(Hz)': 162.568,\n",
    "    'MDVP:Fhi(Hz)': 198.346,\n",
    "    'MDVP:Flo(Hz)': 77.63,\n",
    "    'MDVP:Jitter(%)': 0.00502,\n",
    "    'MDVP:Jitter(Abs)': 0.00003,\n",
    "    'MDVP:RAP': 0.0028,\n",
    "    'MDVP:PPQ': 0.00253,\n",
    "    'Jitter:DDP': 0.00841,\n",
    "    'MDVP:Shimmer': 0.01791,\n",
    "    'MDVP:Shimmer(dB)': 0.168,\n",
    "    'Shimmer:APQ3': 0.00793,\n",
    "    'Shimmer:APQ5': 0.01057,\n",
    "    'MDVP:APQ': 0.01799,\n",
    "    'Shimmer:DDA': 0.0238,\n",
    "    'NHR': 0.0117,\n",
    "    'HNR': 25.678\n",
    "}\n",
    "\n",
    "#for status = 0\n",
    "status = 0\n",
    "evidence = {\n",
    "    'MDVP:Fo(Hz)': 197.076,\n",
    "    'MDVP:Fhi(Hz)': 206.896,\n",
    "    'MDVP:Flo(Hz)': 192.055,\n",
    "    'MDVP:Jitter(%)': 0.00289,\n",
    "    'MDVP:Jitter(Abs)': 0.00001,\n",
    "    'MDVP:RAP': 0.00166,\n",
    "    'MDVP:PPQ': 0.00168,\n",
    "    'Jitter:DDP': 0.00498,\n",
    "    'MDVP:Shimmer': 0.01098,\n",
    "    'MDVP:Shimmer(dB)': 0.097,\n",
    "    'Shimmer:APQ3': 0.00563,\n",
    "    'Shimmer:APQ5': 0.0068,\n",
    "    'MDVP:APQ': 0.00802,\n",
    "    'Shimmer:DDA': 0.01689,\n",
    "    'NHR': 0.00339,\n",
    "    'HNR': 26.775\n",
    "}\n",
    "\n",
    "\n",
    "# Load your training dataset to get the binning information\n",
    "data = pd.read_csv('./parkinsons_data-og.csv')\n",
    "data = data.drop(columns=['name'])\n",
    "\n",
    "# Define number of bins (should match training configuration)\n",
    "n_bins = 4\n",
    "\n",
    "# Discretize evidence to match trained model bins\n",
    "for col, value in evidence.items():\n",
    "    if col in data.columns:\n",
    "        # Get bin edges for this column\n",
    "        bins = pd.cut(data[col], bins=n_bins, labels=False, retbins=True)[1]\n",
    "        \n",
    "        # Determine which bin the evidence value falls into\n",
    "        evidence[col] = np.digitize(value, bins) - 1  # \"-1\" to zero-index\n",
    "\n",
    "# Perform the query for P(status=1 | evidence)\n",
    "try:\n",
    "    prob_result = inference.query(variables=['status'], evidence=evidence)\n",
    "    prob_status_1 = prob_result.values[status]  # Probability of status=1\n",
    "    print(f\"P(status={status} | evidence) = {prob_status_1:.6f}\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}. Please check if evidence is within the trained model's range.\")\n",
    "    \n",
    "print(bayesian_model.get_cardinality())  # Print the cardinalities of all variables in the model\n",
    "\n",
    "    \n",
    "# After cross-validation, train on the entire dataset for final evaluation or inference\n",
    "final_hc = HillClimbSearch(data)\n",
    "final_model = final_hc.estimate(scoring_method=BDeuScore(data), max_iter=max_iter)\n",
    "final_bayesian_model = BayesianNetwork(final_model.edges())\n",
    "final_bayesian_model.fit(data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "print(evidence)\n",
    "# Perform inference using the final model\n",
    "inference = VariableElimination(final_bayesian_model)\n",
    "prob_result = inference.query(variables=['status'], evidence=evidence)\n",
    "print(f\"P(status=1 | evidence) = {prob_result.values[1]:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score,\n",
    "    brier_score_loss\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "from pgmpy.estimators import HillClimbSearch, BDeuScore, MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Factor class to store CPTs and perform operations\n",
    "class Factor:\n",
    "    def __init__(self, variables, cpt):\n",
    "        self.variables = variables  # List of variables involved in the factor\n",
    "        self.cpt = cpt  # Conditional probability table (CPT)\n",
    "\n",
    "    def marginalize(self, variable):\n",
    "        \"\"\"Marginalize out the variable from this factor.\"\"\"\n",
    "        if variable not in self.variables:\n",
    "            return self  # No need to marginalize if the variable is not in this factor\n",
    "        \n",
    "        if isinstance(self.variables, set):\n",
    "            self.variables = list(self.variables)  # Convert to list if it's a set\n",
    "\n",
    "        var_idx = self.variables.index(variable)  # Now this should work\n",
    "        new_vars = self.variables[:var_idx] + self.variables[var_idx+1:]\n",
    "        new_cpt = np.sum(self.cpt, axis=var_idx)\n",
    "        return Factor(new_vars, new_cpt)\n",
    "\n",
    "\n",
    "    def restrict(self, variable, value):\n",
    "        \"\"\" Restrict the factor to a specific value of the variable \"\"\"\n",
    "        print('variable',variable)\n",
    "        print('self.variables',self.variables)\n",
    "        print(\"value\", value)\n",
    "        print(\"variable not in self.variables\",variable not in self.variables)\n",
    "    \n",
    "        if variable not in self.variables:\n",
    "            # print(f\"Variable '{variable}' not found in factor variables: {self.variables}\")\n",
    "            return self  # No need to restrict if the variable is not in this factor\n",
    "        \n",
    "        var_idx = self.variables.index(variable)\n",
    "        new_vars = self.variables[:var_idx] + self.variables[var_idx+1:]\n",
    "        # print(\"variable\",variable)\n",
    "        print(\"new_vars\",new_vars)\n",
    "        \n",
    "        print(\"var_idx\",var_idx)\n",
    "        # print(\"self.variables\",self.variables)\n",
    "        # print(\"value\",value)\n",
    "        value_idx = np.where(self.variables[var_idx] == value)[0]\n",
    "        print('value_idx',value_idx)\n",
    "        \n",
    "        new_cpt = self.cpt.take(value_idx, axis=var_idx)\n",
    "        print(new_cpt)\n",
    "        t = t[0]\n",
    "        return Factor(new_vars, new_cpt)\n",
    "\n",
    "\n",
    "class VariableElimination:\n",
    "    def __init__(self, factors):\n",
    "        self.factors = factors\n",
    "\n",
    "    def eliminate(self, variable):\n",
    "        \"\"\"Eliminate a variable by summing over all its possible values.\"\"\"\n",
    "        relevant_factors = [factor for factor in self.factors if variable in factor.variables]\n",
    "        \n",
    "        if not relevant_factors:  # Check if no relevant factors were found\n",
    "            print(f\"No relevant factors found for variable {variable}. Skipping elimination.\")\n",
    "            return\n",
    "        \n",
    "        combined_factor = self.combine_factors(relevant_factors)\n",
    "        marginalized_factor = combined_factor.marginalize(variable)\n",
    "        self.factors = [factor for factor in self.factors if factor not in relevant_factors]\n",
    "        self.factors.append(marginalized_factor)\n",
    "\n",
    "\n",
    "    def combine_factors(self, factors):\n",
    "        \"\"\" Combine a list of factors by multiplying their CPTs \"\"\"\n",
    "        # print(\"factors\",factors)\n",
    "        if not factors:  # Check if factors is empty\n",
    "            raise ValueError(\"No relevant factors found for combining.\")\n",
    "        \n",
    "        common_vars = set(factors[0].variables)\n",
    "        for factor in factors[1:]:\n",
    "            common_vars.intersection_update(factor.variables)\n",
    "        \n",
    "        combined_cpt = None\n",
    "        for factor in factors:\n",
    "            if combined_cpt is None:\n",
    "                combined_cpt = factor.cpt\n",
    "            else:\n",
    "                combined_cpt = np.multiply(combined_cpt, factor.cpt)\n",
    "        \n",
    "        return Factor(common_vars, combined_cpt)\n",
    "\n",
    "    def query(self, query_variables, evidence={}):\n",
    "        \"\"\" Perform a query given a set of query variables and evidence \"\"\"\n",
    "        for variable, value in evidence.items():\n",
    "            for factor in self.factors:\n",
    "                factor = factor.restrict(variable, value)\n",
    "        \n",
    "        print('factor',factor.variables)\n",
    "        t = t[0]\n",
    "        \n",
    "        non_query_variables = [var for factor in self.factors for var in factor.variables if var not in query_variables]\n",
    "        for var in non_query_variables:\n",
    "            self.eliminate(var)\n",
    "        \n",
    "        joint_cpt = None\n",
    "        for factor in self.factors:\n",
    "            if joint_cpt is None:\n",
    "                joint_cpt = factor.cpt\n",
    "            else:\n",
    "                joint_cpt = np.multiply(joint_cpt, factor.cpt)\n",
    "        \n",
    "        return joint_cpt / np.sum(joint_cpt)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_cpt(data, parent, child, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate the Conditional Probability Table (CPT) for a parent-child pair.\n",
    "    \n",
    "    :param data: DataFrame containing the dataset\n",
    "    :param parent: The parent variable (column name)\n",
    "    :param child: The child variable (column name)\n",
    "    :param n_bins: Number of bins for discretizing continuous data\n",
    "    :return: A CPT (numpy array) for the given parent-child pair\n",
    "    \"\"\"\n",
    "    # Discretize continuous variables by binning them (if necessary)\n",
    "    data[parent] = pd.cut(data[parent], bins=n_bins, labels=False, include_lowest=True)\n",
    "    data[child] = pd.cut(data[child], bins=n_bins, labels=False, include_lowest=True)\n",
    "    \n",
    "    # Initialize a dictionary to store the joint frequencies\n",
    "    joint_freqs = defaultdict(int)\n",
    "    \n",
    "    # Calculate joint frequency of (parent_value, child_value) pairs\n",
    "    for _, row in data.iterrows():\n",
    "        parent_val = row[parent]\n",
    "        child_val = row[child]\n",
    "        joint_freqs[(parent_val, child_val)] += 1\n",
    "    \n",
    "    # Get the total number of samples for normalization\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    # Initialize the CPT matrix\n",
    "    cpt = np.zeros((n_bins, n_bins))  # n_bins for both parent and child\n",
    "    \n",
    "    # Fill the CPT with conditional probabilities P(child | parent)\n",
    "    for parent_val in range(n_bins):\n",
    "        parent_count = sum(1 for val in data[parent] if val == parent_val)\n",
    "        if parent_count == 0:\n",
    "            continue  # Skip if no samples for this parent value\n",
    "        \n",
    "        for child_val in range(n_bins):\n",
    "            joint_count = joint_freqs[(parent_val, child_val)]\n",
    "            cpt[parent_val, child_val] = joint_count / parent_count  # P(child_val | parent_val)\n",
    "    \n",
    "    return cpt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./parkinsons_data-og.csv')\n",
    "data = data.drop(columns=['name'])\n",
    "\n",
    "# Preprocess data: Handle missing values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Discretize continuous variables (example with 4 bins for each feature)\n",
    "\n",
    "n_bins = 4\n",
    "for col in data.columns:\n",
    "    if data[col].dtype != 'object':\n",
    "        data[col] = pd.cut(data[col], bins=n_bins, labels=False)\n",
    "\n",
    "# Encode categorical columns if any (convert to integer categories)\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    \n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# To store results for all metrics\n",
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "f1_scores = []\n",
    "roc_aucs = []\n",
    "brier_scores = []\n",
    "kl_divergences = []\n",
    "max_iter = 200000000000\n",
    "\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data = data.iloc[train_index]\n",
    "    test_data = data.iloc[test_index]\n",
    "\n",
    "    # Structure learning using Hill Climb and BDeu score\n",
    "    hc = HillClimbSearch(train_data)\n",
    "    model = hc.estimate(scoring_method=BDeuScore(train_data), max_iter=max_iter)  # Use BDeu score\n",
    "    \n",
    "    # Example: Iterate over all edges in the model to calculate the CPT for each edge\n",
    "    factors = []\n",
    "\n",
    "    for edge in model.edges():\n",
    "        parent, child = edge\n",
    "        cpt = calculate_cpt(data, parent, child, n_bins)  # Assuming you want 10 bins for discretization\n",
    "        factors.append(Factor([parent, child], cpt))\n",
    "   \n",
    "    # Set up Variable Elimination\n",
    "    ve = VariableElimination(factors)\n",
    "    # Lists to store true and predicted values for metrics\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    # Testing the model\n",
    "    for _, row in test_data.iterrows():\n",
    "        evidence = row.drop('status').to_dict()  # Modify target variable if needed\n",
    "        true_status = row['status']\n",
    "        # Filter evidence to only include variables that exist in the factors\n",
    "        valid_evidence = {}\n",
    "        for var, value in evidence.items():\n",
    "            for factor in factors:\n",
    "                if var in factor.variables:\n",
    "                    valid_evidence[var] = value\n",
    "                    break  # Only include evidence for variables present in the factor list\n",
    "        # Perform inference to predict 'status'\n",
    "        try:\n",
    "            query_result = ve.query(query_variables=['status'], evidence=valid_evidence)\n",
    "            print(\"query_result\",query_result)\n",
    "            t=t[0]\n",
    "            y_true.append(true_status)\n",
    "            y_pred.append(np.argmax(query_result))  # Assuming 2 classes, take the max probability\n",
    "\n",
    "            prob_demented = query_result[1]  # Probability of class 1 (Demented)\n",
    "            y_prob.append(prob_demented)\n",
    "        \n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError for evidence: {valid_evidence}. Missing key: {e}\")\n",
    "            y_prob.append(0.0000001)  # Append a very small value to avoid NaN\n",
    "\n",
    "            \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # Check for NaN values and filter out any rows with NaNs\n",
    "    valid_indices = ~np.isnan(y_true) & ~np.isnan(y_prob)\n",
    "    y_true_clean = y_true[valid_indices]\n",
    "    y_pred_clean = y_pred[valid_indices]\n",
    "    y_prob_clean = y_prob[valid_indices]\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracies.append(accuracy_score(y_true_clean, y_pred_clean))\n",
    "    balanced_accuracies.append(balanced_accuracy_score(y_true_clean, y_pred_clean))\n",
    "    f1_scores.append(f1_score(y_true_clean, y_pred_clean, pos_label=0, average='weighted'))\n",
    "    \n",
    "    \n",
    "    if len(y_prob_clean) > 0:  # Ensure there are valid probabilities\n",
    "        roc_aucs.append(roc_auc_score(y_true_clean, y_prob_clean))\n",
    "        brier_scores.append(brier_score_loss(y_true_clean, y_prob_clean))\n",
    "        \n",
    "        # Calculate KL Divergence for this fold\n",
    "        epsilon = 1e-10\n",
    "        prob_true = np.array([p if y == 1 else 1 - p for y, p in zip(y_true_clean, y_prob_clean)])\n",
    "        prob_true = np.clip(prob_true, epsilon, None)  # Ensure no zero probabilities\n",
    "        y_prob_clean = np.clip(y_prob_clean, epsilon, None)  # Ensure no zero probabilities\n",
    "\n",
    "        kl_div = entropy(prob_true, y_prob_clean, base=2)\n",
    "        kl_divergences.append(kl_div)\n",
    "\n",
    "# Average the results across folds\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Balanced Accuracy: {np.mean(balanced_accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"ROC AUC: {np.mean(roc_aucs):.4f}\")\n",
    "print(f\"Brier Score: {np.mean(brier_scores):.4f}\")\n",
    "print(f\"KL Divergence: {np.mean(kl_divergences):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score,\n",
    "    brier_score_loss\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "from pgmpy.estimators import HillClimbSearch, BDeuScore, BayesianEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./parkinsons_data-og.csv')\n",
    "data = data.drop(columns=['name'])\n",
    "\n",
    "# Preprocess data: Handle missing values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Discretize continuous variables (example with 4 bins for each feature)\n",
    "n_bins = 4\n",
    "for col in data.columns:\n",
    "    if data[col].dtype != 'object':\n",
    "        data[col] = pd.cut(data[col], bins=n_bins, labels=False)\n",
    "\n",
    "# Encode categorical columns if any (convert to integer categories)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# To store results for all metrics\n",
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "f1_scores = []\n",
    "roc_aucs = []\n",
    "brier_scores = []\n",
    "kl_divergences = []\n",
    "max_iter = 200000000000\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data = data.iloc[train_index]\n",
    "    test_data = data.iloc[test_index]\n",
    "\n",
    "    # Structure learning using Hill Climb and BDeu score\n",
    "    hc = HillClimbSearch(train_data)\n",
    "    model = hc.estimate(scoring_method=BDeuScore(train_data), max_iter=max_iter)  # Use BDeu score\n",
    "    \n",
    "    # Define Bayesian Network with learned structure and fit CPTs using BayesianEstimator with Laplace smoothing\n",
    "    bayesian_model = BayesianNetwork(model.edges())\n",
    "    bayesian_model.fit(train_data, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=1)\n",
    "\n",
    "    # Inference setup\n",
    "    inference = VariableElimination(bayesian_model)\n",
    "    with open('./bayesian_model.pkl', 'wb') as f:\n",
    "        pickle.dump(bayesian_model, f)\n",
    "    \n",
    "    # Lists to store true and predicted values for metrics\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    # Testing the model\n",
    "    for _, row in test_data.iterrows():\n",
    "        evidence = row.drop('status').to_dict()  # Modify target variable if needed\n",
    "        true_status = row['status']\n",
    "        \n",
    "        # Filter evidence to only include values within valid states for each variable\n",
    "        valid_evidence = {var: value for var, value in evidence.items() if value < n_bins}\n",
    "        \n",
    "        # Perform inference to predict 'status'\n",
    "        try:\n",
    "            query_result = inference.map_query(variables=['status'], evidence=valid_evidence)\n",
    "            y_true.append(true_status)\n",
    "            y_pred.append(query_result['status'])\n",
    "\n",
    "            try:\n",
    "                # Perform the probability query\n",
    "                prob_result = inference.query(variables=['status'], evidence=valid_evidence)\n",
    "                prob_demented = prob_result.values[1]  # Probability of class 1\n",
    "                y_prob.append(prob_demented)\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError for evidence: {valid_evidence}. Missing key: {e}\")\n",
    "                y_prob.append(0.0000001)  # Append a very small value to avoid NaN\n",
    "            \n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # Filter valid rows without NaNs\n",
    "    valid_indices = ~np.isnan(y_true) & ~np.isnan(y_prob)\n",
    "    y_true_clean = y_true[valid_indices]\n",
    "    y_pred_clean = y_pred[valid_indices]\n",
    "    y_prob_clean = y_prob[valid_indices]\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    unique_labels = set(y_true_clean) | set(y_pred_clean)\n",
    "    pos_label = 1 if 1 in unique_labels else min(unique_labels)\n",
    "\n",
    "    accuracies.append(accuracy_score(y_true_clean, y_pred_clean))\n",
    "    balanced_accuracies.append(balanced_accuracy_score(y_true_clean, y_pred_clean))\n",
    "    f1_scores.append(f1_score(y_true_clean, y_pred_clean, pos_label=pos_label))\n",
    "    \n",
    "    if len(y_prob_clean) > 0:\n",
    "        roc_aucs.append(roc_auc_score(y_true_clean, y_prob_clean))\n",
    "        brier_scores.append(brier_score_loss(y_true_clean, y_prob_clean))\n",
    "        \n",
    "        # KL Divergence for this fold\n",
    "        epsilon = 1e-10\n",
    "        prob_true = np.array([p if y == pos_label else 1 - p for y, p in zip(y_true_clean, y_prob_clean)])\n",
    "        prob_true = np.clip(prob_true, epsilon, None)\n",
    "        y_prob_clean = np.clip(y_prob_clean, epsilon, None)\n",
    "\n",
    "        kl_div = entropy(prob_true, y_prob_clean, base=2)\n",
    "        kl_divergences.append(kl_div)\n",
    "\n",
    "# Average the results across folds\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Balanced Accuracy: {np.mean(balanced_accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"ROC AUC: {np.mean(roc_aucs):.4f}\")\n",
    "print(f\"Brier Score: {np.mean(brier_scores):.4f}\")\n",
    "print(f\"KL Divergence: {np.mean(kl_divergences):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
