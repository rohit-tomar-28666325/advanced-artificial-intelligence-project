{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0(MDVP_Fo_Hz);X1(MDVP_Fhi_Hz);X2(MDVP_Flo_Hz);X3(MDVP_Jitter_%);X4(MDVP_Jitter_Abs);X5(MDVP_RAP);X6(MDVP_PPQ);X7(Jitter_DDP);X8(MDVP_Shimmer);X9(MDVP_Shimmer_dB);X10(Shimmer_APQ3);X11(Shimmer_APQ5);X12(MDVP_APQ);X13(Shimmer_DDA);X14(NHR);X15(HNR);X16(status);X17(RPDE);X18(DFA);X19(spread1);X20(spread2);X21(D2);X22(PPE);\n",
      "P(MDVP_Fo_Hz|status);P(MDVP_Fhi_Hz|status);P(MDVP_Flo_Hz|status);P(MDVP_Jitter_%|status);P(MDVP_Jitter_Abs|status);P(MDVP_RAP|status);P(MDVP_PPQ|status);P(Jitter_DDP|status);P(MDVP_Shimmer|status);P(MDVP_Shimmer_dB|status);P(Shimmer_APQ3|status);P(Shimmer_APQ5|status);P(MDVP_APQ|status);P(Shimmer_DDA|status);P(NHR|status);P(HNR|status);P(status|status);P(RPDE|status);P(DFA|status);P(spread1|status);P(spread2|status);P(D2|status);P(PPE|status);\n"
     ]
    }
   ],
   "source": [
    "s=\"MDVP:Fo(Hz),MDVP:Fhi(Hz),MDVP:Flo(Hz),MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP,MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA,NHR,HNR,status,RPDE,DFA,spread1,spread2,D2,PPE\"\n",
    "s = s.split(\",\")\n",
    "\n",
    "n1 = \"\"\n",
    "n2 = \"\"\n",
    "i=0\n",
    "t = \"status\"\n",
    "for k in s:\n",
    "    k = k.replace(\"(\",\"_\")\n",
    "    k = k.replace(\":\",\"_\")\n",
    "    k = k.replace(\")\",\"\")\n",
    "    n1+= f'X{i}({k});'\n",
    "    n2+= f'P({k}|{t});'\n",
    "    i+=1\n",
    "    \n",
    "print(n1)\n",
    "print(n2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  MDVP:Jitter(Abs)  \\\n",
      "0      119.992       157.302        74.997         0.00784           0.00007   \n",
      "1      122.400       148.650       113.819         0.00968           0.00008   \n",
      "2      116.682       131.111       111.555         0.01050           0.00009   \n",
      "3      116.676       137.871       111.366         0.00997           0.00009   \n",
      "4      116.014       141.781       110.655         0.01284           0.00011   \n",
      "\n",
      "   MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  MDVP:Shimmer(dB)  ...  \\\n",
      "0   0.00370   0.00554     0.01109       0.04374             0.426  ...   \n",
      "1   0.00465   0.00696     0.01394       0.06134             0.626  ...   \n",
      "2   0.00544   0.00781     0.01633       0.05233             0.482  ...   \n",
      "3   0.00502   0.00698     0.01505       0.05492             0.517  ...   \n",
      "4   0.00655   0.00908     0.01966       0.06425             0.584  ...   \n",
      "\n",
      "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
      "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
      "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
      "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
      "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
      "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
      "\n",
      "    spread2        D2       PPE  \n",
      "0  0.266482  2.301442  0.284654  \n",
      "1  0.335590  2.486855  0.368674  \n",
      "2  0.311173  2.342259  0.332634  \n",
      "3  0.334147  2.405554  0.368975  \n",
      "4  0.234513  2.332180  0.410335  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195 entries, 0 to 194\n",
      "Data columns (total 23 columns):\n",
      " #   Column           Non-Null Count  Dtype\n",
      "---  ------           --------------  -----\n",
      " 0   NHR              195 non-null    int32\n",
      " 1   HNR              195 non-null    int32\n",
      " 2   status           195 non-null    int64\n",
      " 3   RPDE             195 non-null    int32\n",
      " 4   DFA              195 non-null    int32\n",
      " 5   spread1          195 non-null    int32\n",
      " 6   spread2          195 non-null    int32\n",
      " 7   D2               195 non-null    int32\n",
      " 8   PPE              195 non-null    int32\n",
      " 9   MDVP_Fo_Hz       195 non-null    int32\n",
      " 10  MDVP_Fhi_Hz      195 non-null    int32\n",
      " 11  MDVP_Flo_Hz      195 non-null    int32\n",
      " 12  MDVP_Jitter_%    195 non-null    int32\n",
      " 13  MDVP_Jitter_Abs  195 non-null    int32\n",
      " 14  MDVP_RAP         195 non-null    int32\n",
      " 15  MDVP_PPQ         195 non-null    int32\n",
      " 16  Jitter_DDP       195 non-null    int32\n",
      " 17  MDVP_Shimmer     195 non-null    int32\n",
      " 18  MDVP_Shimmer_dB  195 non-null    int32\n",
      " 19  Shimmer_APQ3     195 non-null    int32\n",
      " 20  Shimmer_APQ5     195 non-null    int32\n",
      " 21  MDVP_APQ         195 non-null    int32\n",
      " 22  Shimmer_DDA      195 non-null    int32\n",
      "dtypes: int32(22), int64(1)\n",
      "memory usage: 18.4 KB\n",
      "None\n",
      "   NHR  HNR  status  RPDE  DFA  spread1  spread2  D2  PPE  MDVP_Fo_Hz  ...  \\\n",
      "0    0    2       1     1    3        2        2   1    1           0  ...   \n",
      "1    0    1       1     1    3        2        2   1    2           0  ...   \n",
      "2    0    1       1     1    3        2        2   1    2           0  ...   \n",
      "3    0    1       1     1    3        2        2   1    2           0  ...   \n",
      "4    0    1       1     1    3        3        2   1    3           0  ...   \n",
      "\n",
      "   MDVP_Jitter_Abs  MDVP_RAP  MDVP_PPQ  Jitter_DDP  MDVP_Shimmer  \\\n",
      "0                0         0         0           0             1   \n",
      "1                1         0         1           0             1   \n",
      "2                1         0         1           0             1   \n",
      "3                1         0         1           0             1   \n",
      "4                1         1         1           1             1   \n",
      "\n",
      "   MDVP_Shimmer_dB  Shimmer_APQ3  Shimmer_APQ5  MDVP_APQ  Shimmer_DDA  \n",
      "0                1             1             1         0            1  \n",
      "1                1             2             2         1            2  \n",
      "2                1             1             1         0            1  \n",
      "3                1             1             1         0            1  \n",
      "4                1             2             2         1            2  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\anaconda3\\envs\\AML\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder\n",
    "import pickle\n",
    "\n",
    "column = ['name', 'MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP', 'MDVP:Shimmer',\n",
    "          'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'status', 'RPDE', 'DFA', 'spread1', 'spread2', 'D2', 'PPE']\n",
    "# Load the dataset\n",
    "file_path = './data/parkinsons_data-og.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary identifier columns\n",
    "data_cleaned = data.drop(columns=[\"name\"])\n",
    "\n",
    "# Drop rows with missing values\n",
    "data_cleaned.dropna(inplace=True)\n",
    "# data_cleaned['Group_v'] = data_cleaned['Group']\n",
    "print(data_cleaned.head())\n",
    "\n",
    "\n",
    "\n",
    "# Discretize continuous variables into limited categories (e.g., 4 bins)\n",
    "continuous_columns = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP', 'MDVP:Shimmer',\n",
    "                      'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'spread1', 'spread2', 'D2', 'PPE']\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')\n",
    "data_cleaned[continuous_columns] = discretizer.fit_transform(\n",
    "    data_cleaned[continuous_columns])\n",
    "\n",
    "for col in continuous_columns:\n",
    "    k = col.replace(\"(\",\"_\")\n",
    "    k = k.replace(\":\",\"_\")\n",
    "    k = k.replace(\")\",\"\")\n",
    "    data_cleaned[k] = data_cleaned[col].astype(int)\n",
    "    if(k != col):\n",
    "        data_cleaned = data_cleaned.drop(columns=[col])\n",
    "\n",
    "\n",
    "\n",
    "# Display the cleaned data's structure and the first few rows to verify\n",
    "print(data_cleaned.info())\n",
    "print(data_cleaned.head())\n",
    "\n",
    "# print('Group------------>',data_cleaned['Group'],'Group_v--------------------------->',data_cleaned['Group_v'])\n",
    "\n",
    "# Save the fitted discretizer to a file\n",
    "with open('./config/discretizer.pkl', 'wb') as f:\n",
    "    pickle.dump(discretizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test datasets saved as 'parkinsons_data_train.csv' and 'parkinsons_data_test.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Define features and target variable\n",
    "X = data_cleaned.drop(columns=['status'])\n",
    "y = data_cleaned['status']\n",
    "\n",
    "#save cleaned data into file\n",
    "data_cleaned.to_csv('./data/parkinsons_data_clean.csv', index=False)\n",
    "\n",
    "\n",
    "# Perform 80:20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Check the distribution of classes in the resampled training set\n",
    "# print(\"Resampled Training set class distribution:\")\n",
    "# print(y_train_resampled.value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# Combine features and target for saving\n",
    "# train_data = pd.concat([X_train_resampled, y_train_resampled], axis=1)\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save to CSV files\n",
    "train_data.to_csv('./data/parkinsons_data_train.csv', index=False)\n",
    "test_data.to_csv('./data/parkinsons_data_test.csv', index=False)\n",
    "\n",
    "print(\"Train and test datasets saved as 'parkinsons_data_train.csv' and 'parkinsons_data_test.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in the full dataset\n",
    "print(\"Full dataset class distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Check class distribution in the training set\n",
    "print(\"Training set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Check class distribution in the testing set\n",
    "print(\"Testing set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, K2Score, HillClimbSearch\n",
    "from pgmpy.factors.continuous import GaussianDistribution\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dementia_data.csv\")  # Replace with your file path\n",
    "\n",
    "# Encode categorical columns if not already encoded\n",
    "categorical_columns = ['Group', 'M/F', 'Hand']  # Example of columns to encode\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up model structure (example)\n",
    "# Define the structure manually or use HillClimbSearch\n",
    "hc = HillClimbSearch(data)\n",
    "best_model = hc.estimate(scoring_method=K2Score(data))\n",
    "\n",
    "# Gaussian Bayesian Network with k-fold cross-validation\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "    # Define the Bayesian model\n",
    "    model = BayesianNetwork(best_model.edges())\n",
    "\n",
    "    # Train the model with Maximum Likelihood Estimation (MLE)\n",
    "    model.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "    # Testing and evaluation can be done here for each fold\n",
    "    # Example: predicting or calculating log-likelihood\n",
    "    # log_likelihood = model.score(test_data)\n",
    "    # print(f\"Log-likelihood for fold: {log_likelihood}\")\n",
    "\n",
    "print(\"Training complete with 5-fold cross-validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Query with continuous values\n",
    "query_continuous = {\n",
    "    \"Visit\": 2, \"Age\": 88, \"EDUC\": 14, \"SES\": 2, 'MR Delay':2,\n",
    "    \"MMSE\": 30, \"CDR\": 0, \"eTIV\": 2004, \"nWBV\": 0.681, \"ASF\": 0.876\n",
    "}\n",
    "\n",
    "# Continuous columns to discretize\n",
    "# continuous_columns = [\"Age\", \"EDUC\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
    "continuous_columns = [\"MR Delay\", \"Age\", \"EDUC\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
    "\n",
    "\n",
    "# Transform continuous query values to discrete bins\n",
    "query_discrete = query_continuous.copy()\n",
    "for col in continuous_columns:\n",
    "    # Use the same discretizer to transform the single value\n",
    "    # Note: KBinsDiscretizer expects a 2D array, hence [[value]]\n",
    "    query_value = [[query_continuous[col]]]  # Shape (1, 1)\n",
    "    print(query_value)\n",
    "    # Use the discretizer to transform the value\n",
    "    query_discrete[col] = int(discretizer.transform(query_value)[0, 0])\n",
    "\n",
    "print(query_discrete)\n",
    "\n",
    "# # Perform the probabilistic query with the discrete values\n",
    "# # For instance, in pgmpy you might use `inference.query`\n",
    "# # Assuming you have an `inference` object from VariableElimination or similar in pgmpy\n",
    "# from pgmpy.inference import VariableElimination\n",
    "\n",
    "# inference = VariableElimination(model)\n",
    "# result = inference.query(\n",
    "#     variables=[\"Group\"],\n",
    "#     evidence=query_discrete\n",
    "# )\n",
    "\n",
    "# print(\"Query Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "\n",
    "query = \"P(Group | Visit=3, Age=80,EDUC=12, MMSE=22, CDR=0.5, eTIV=1698, nWBV=0.701,ASF= 1.034)\"\n",
    "\n",
    "def continuous_to_discrete_query(query):\n",
    "    continuous_columns = [\"MR Delay\", \"Age\", \"EDUC\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
    "    discretizer_file_path = './config/discretizer.pkl'\n",
    "    \n",
    "    # Load the fitted discretizer from the file\n",
    "    with open(discretizer_file_path, 'rb') as f:\n",
    "        discretizer = pickle.load(f)\n",
    "\n",
    "    return make_query_discrete(query, continuous_columns, discretizer)\n",
    "\n",
    "def make_query_discrete(query, continuous_columns, discretizer):\n",
    "    query = query.replace(\" \", \"\")\n",
    "    target = query.split(\"|\")[0].replace(\"P(\", \"\")\n",
    "    evedences = query.split(\"|\")[1].replace(\")\", \"\").split(\",\")\n",
    "\n",
    "    query_continuous = {}\n",
    "    for ev in evedences:\n",
    "        key, value = ev.split(\"=\")\n",
    "        query_continuous[key] = value\n",
    "\n",
    "    # Create a DataFrame with a single row for the discretizer\n",
    "    query_df = pd.DataFrame([query_continuous], columns=continuous_columns).fillna(0)\n",
    "\n",
    "    # Transform the continuous query values to discrete bins\n",
    "    transformed = discretizer.transform(query_df)  # Shape (1, n_features)\n",
    "\n",
    "    # Create a new dictionary to hold discrete values\n",
    "    query_discrete = query_continuous.copy()\n",
    "    for i, col in enumerate(continuous_columns):\n",
    "        if col in query_discrete:\n",
    "            query_discrete[col] = int(transformed[0, i])\n",
    "\n",
    "    ev = [f'{key}={query_discrete[key]}' for key in query_discrete]\n",
    "    print(f'P({target}|{\",\".join(ev)})')\n",
    "    \n",
    "    return f'P({target}|{\",\".join(ev)})'\n",
    "\n",
    "continuous_to_discrete_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the edges based on the provided relationships\n",
    "edges = [\n",
    "    ('Group', 'CDR'),\n",
    "    ('Group', 'SES'),\n",
    "    ('Visit', 'MR_Delay'),\n",
    "    ('MR_Delay', 'Group'),\n",
    "    ('M/F', 'Age'),\n",
    "    ('SES', 'EDUC'),\n",
    "    ('SES', 'M/F'),\n",
    "    ('CDR', 'MMSE'),\n",
    "    ('CDR', 'nWBV'),\n",
    "    ('eTIV', 'M/F'),\n",
    "    ('eTIV', 'SES'),\n",
    "    ('nWBV', 'Age'),\n",
    "    ('nWBV', 'ASF'),\n",
    "    ('ASF', 'eTIV')\n",
    "]\n",
    "\n",
    "# Add edges to the graph\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_color='black', font_weight='bold', arrows=True, arrowstyle='-|>')\n",
    "plt.title(\"Bayesian Network Structure\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\"P(Group|Visit=3,Age=2,EDUC=1,MMSE=2,CDR=1,eTIV=2,nWBV=1,ASF=0\"\n",
    "\"P(Group|Visit=2,Age=1,EDUC=1,SES=2,MMSE=3,CDR=0,eTIV=3,nWBV=0,ASF=0)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTING performance on test data...\n",
    "Balanced Accuracy=0.7070065445746027\n",
    "F1 Score=0.8687423451654547\n",
    "Area Under Curve=0.4664378337147216\n",
    "Brier Score=0.013880668852643482\n",
    "KL Divergence=99.79329425545026\n",
    "Training Time=this number should come from the CPT_Generator!\n",
    "Inference Time=0.09596109390258789 secs.\n",
    "\n",
    "Model -> \n",
    "structure:P(Visit);P(MR_Delay|Visit);P(Group|MR_Delay);P(CDR|Group);P(MMSE|CDR);P(nWBV|CDR);P(ASF|nWBV);P(Age|nWBV,M/F);P(eTIV|ASF);P(M/F|eTIV,SES);P(SES|EDUC);P(EDUC|eTIV,Group)\n",
    "\n",
    "\n",
    "COMPUTING performance on test data...\n",
    "Balanced Accuracy=0.7272727272727272\n",
    "F1 Score=0.8886833494723215\n",
    "Area Under Curve=0.46491228070175433\n",
    "Brier Score=0.010097246474140025\n",
    "KL Divergence=98.62747800959588\n",
    "Training Time=this number should come from the CPT_Generator!\n",
    "Inference Time=0.102325439453125 secs.\n",
    "\n",
    "\n",
    "COMPUTING performance on test data...\n",
    "Balanced Accuracy=0.95\n",
    "F1 Score=0.953023019340545\n",
    "Area Under Curve=0.9715789473684211\n",
    "Brier Score=0.2507101004634838\n",
    "KL Divergence=14.168921981535366\n",
    "Training Time=this number should come from the CPT_Generator!\n",
    "Inference Time=0.10036468505859375 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "data1 = [1, 2, 2, 1, 2, 1, 2, 0, 1, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 0, 0, \n",
    "         2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 2, 2, 2, 1, 1, \n",
    "         1, 2, 1, 2, 2, 1, 2, 2, 0, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 0, 2, 1, 2, \n",
    "         1, 2, 2, 0, 0, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, \n",
    "         1, 2, 2, 1, 2, 2, 0, 2, 1, 1, 1]\n",
    "data2 = [1, 2, 2, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, \n",
    "         2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, \n",
    "         1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, \n",
    "         1, 2, 2, 0, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 0, 1, 2, \n",
    "         1, 2, 2, 0, 0, 2, 1, 2, 1, 1, 1]\n",
    "\n",
    "# Plotting the histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data1, bins=[0, 1, 2, 3], edgecolor='black', align='left', rwidth=0.4, label=\"Data 1\", color='blue', alpha=0.6)\n",
    "plt.hist(data2, bins=[0, 1, 2, 3], edgecolor='black', align='left', rwidth=0.4, label=\"Data 2\", color='orange', alpha=0.6)\n",
    "\n",
    "# Labeling\n",
    "plt.xticks([0, 1, 2])\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Side-by-Side Histogram of Data 1 and Data 2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ground truth and predictions\n",
    "Y_true = [1, 2, 2, 1, 2, 1, 2, 0, 1, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 0, 0, \n",
    "          2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 2, 2, 2, 1, 1, \n",
    "          1, 2, 1, 2, 2, 1, 2, 2, 0, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 0, 2, 1, 2, \n",
    "          1, 2, 2, 0, 0, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, \n",
    "          1, 2, 2, 1, 2, 2, 0, 2, 1, 1, 1]\n",
    "\n",
    "Y_prob = [0.9962598199975182, 0.9743738690083193, 0.992684388582693, 0.7565328605888396, 0.9906154310497827, \n",
    "          0.9916897778202836, 0.9753381884152057, 0.18108373960158297, 0.9371020521938085, 0.8172873961037203, \n",
    "          0.6652793931113513, 0.9867798855010383, 0.9014646462719007, 0.85821400157766, 0.9750268217524632, \n",
    "          0.9666362222175585, 0.9896853350989585, 0.9473763527001476, 0.9899038703954912, 0.8953474042182066, \n",
    "          0.890621712094065, 0.9676959099830867, 0.9691662539401538, 0.9614182257894318, 0.9524323016609413, \n",
    "          0.5321453939545466, 0.5107723392991436, 0.89366541560749, 0.9758209879890276, 0.9996987318135901, \n",
    "          0.9585298516859041, 0.9723901032971002, 0.9998715922638235, 0.9967407282495099, 0.9997668128524544, \n",
    "          0.984009195612114, 0.948890721989006, 0.9871779092484755, 0.9331685165268537, 0.7948648212560112, \n",
    "          0.9774840468187133, 0.8885429564619552, 0.7661093389890649, 0.9329843845907695, 0.7176894066662481, \n",
    "          0.9820239634021523, 0.9022565535789038, 0.8252562645773428, 0.9071512495631361, 0.9769760599559744, \n",
    "          0.7653243331812172, 0.6511782445768378, 0.7151773130385338, 0.9962874086447525, 0.9321314315563827, \n",
    "          0.8133851201933091, 0.990872047078806, 0.8757047699561945, 0.8909695764841207, 0.919609783346365, \n",
    "          0.9099174476755569, 0.9358740781294698, 0.9583044131524623, 0.9738231466913626, 0.6589469402388433, \n",
    "          0.9861719488186033, 0.7426117578169404, 0.9922615111674553, 0.6712379691904601, 0.9533150527663942, \n",
    "          0.9975291270320359, 0.9424019182727988, 0.9993935036620413, 0.9633277196899632, 0.9803368243375562, \n",
    "          0.41595625323385876, 0.5795960957376405, 0.8409507917639985, 0.971318362858906, 0.9816966523778162, \n",
    "          0.9959123843480118, 0.9736850957650526, 0.8717757908490616, 0.9291445393947988, 0.8863662175646968, \n",
    "          0.762858553835607, 0.973686303252637, 0.98085203615549, 0.9034383233400777, 0.9790718334039221, \n",
    "          0.9797272804718148, 0.9702499514692483, 0.9171995968161074, 0.3512185171548708, 0.7555023041782001, \n",
    "          0.9597321852266674, 0.8021814212438095, 0.9475307939588354, 0.8043592682354543, 0.37921812198157734, \n",
    "          0.27791870364840826, 0.9626439188338239, 0.9665277415635528, 0.9569334921022936, 0.9226177172009167, \n",
    "          0.901078604361788, 0.9978869316783254]\n",
    "\n",
    "# Reshape probabilities for each class\n",
    "Y_prob = np.array(Y_prob).reshape(-1, 1)  # Reshape into a 2D array\n",
    "Y_prob = np.hstack((1 - Y_prob, Y_prob * 0.5, Y_prob))\n",
    "\n",
    "# Binarize the true labels\n",
    "Y_true_binarized = label_binarize(Y_true, classes=[0, 1, 2])\n",
    "\n",
    "# Calculate AUC for each class and average macro\n",
    "auc = roc_auc_score(Y_true_binarized, Y_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "print(\"Multiclass AUC (macro-averaged):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseEstimator.__init__() got an unexpected keyword argument 'scoring_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     10\u001b[0m random_variables \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMDVP_Fo_Hz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMDVP_Fhi_Hz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMDVP_Flo_Hz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMDVP_Jitter_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMDVP_Jitter_Abs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMDVP_RAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMDVP_PPQ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJitter_DDP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDFA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspread1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspread2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Step 1: Hill Climbing with BIC or K2 score\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize the Hill Climb search and score method\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m hc \u001b[38;5;241m=\u001b[39m \u001b[43mHillClimbSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBicScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or use K2Score(data) for an alternative\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply Hill Climbing to find the best model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m best_model_hc \u001b[38;5;241m=\u001b[39m hc\u001b[38;5;241m.\u001b[39mestimate()\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\AML\\lib\\site-packages\\pgmpy\\estimators\\HillClimbSearch.py:53\u001b[0m, in \u001b[0;36mHillClimbSearch.__init__\u001b[1;34m(self, data, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m use_cache\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28msuper\u001b[39m(HillClimbSearch, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\AML\\lib\\site-packages\\pgmpy\\estimators\\base.py:291\u001b[0m, in \u001b[0;36mStructureEstimator.__init__\u001b[1;34m(self, data, independencies, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindependencies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindependencies\u001b[38;5;241m.\u001b[39mget_all_variables()\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28msuper\u001b[39m(StructureEstimator, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseEstimator.__init__() got an unexpected keyword argument 'scoring_method'"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import HillClimbSearch, BicScore, K2Score, MmhcEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import ExhaustiveSearch\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('./data/discrete_dementia_data.csv')\n",
    "\n",
    "# Separate target variable if needed, or include all as random variables\n",
    "random_variables = [\n",
    "    \"MDVP_Fo_Hz\", \"MDVP_Fhi_Hz\", \"MDVP_Flo_Hz\", \"MDVP_Jitter_%\", \n",
    "    \"MDVP_Jitter_Abs\", \"MDVP_RAP\", \"MDVP_PPQ\", \"Jitter_DDP\", \n",
    "    \"MDVP_Shimmer\", \"MDVP_Shimmer_dB\", \"Shimmer_APQ3\", \"Shimmer_APQ5\", \n",
    "    \"MDVP_APQ\", \"Shimmer_DDA\", \"NHR\", \"HNR\", \"status\", \"RPDE\", \n",
    "    \"DFA\", \"spread1\", \"spread2\", \"D2\", \"PPE\"\n",
    "]\n",
    "\n",
    "# Step 1: Hill Climbing with BIC or K2 score\n",
    "# Initialize the Hill Climb search and score method\n",
    "hc = HillClimbSearch(data, scoring_method=BicScore(data))  # or use K2Score(data) for an alternative\n",
    "\n",
    "# Apply Hill Climbing to find the best model\n",
    "best_model_hc = hc.estimate()\n",
    "print(\"Hill Climbing Structure Edges:\", best_model_hc.edges())\n",
    "\n",
    "# Step 2: Max-Min Hill Climbing (MMHC) Structure\n",
    "# Initialize the MMHC estimator\n",
    "mmhc = MmhcEstimator(data)\n",
    "\n",
    "# Learn the skeleton using constraint-based part of MMHC\n",
    "skeleton = mmhc.mmpc()\n",
    "\n",
    "# Learn the full Bayesian Network using Hill Climbing on the MMHC skeleton\n",
    "best_model_mmhc = hc.estimate(tabu_length=5, start=skeleton)\n",
    "print(\"MMHC Structure Edges:\", best_model_mmhc.edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the MMHC structure\n",
    "nx.draw(best_model_mmhc, with_labels=True, node_size=2000, font_size=10)\n",
    "plt.title(\"MMHC Bayesian Network Structure\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Field elements must be 2- or 3-tuples, got '1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m])\n\u001b[0;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(a)\n",
      "\u001b[1;31mTypeError\u001b[0m: Field elements must be 2- or 3-tuples, got '1'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1, 2, 3],[1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "result = np.linalg.inv(a)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
